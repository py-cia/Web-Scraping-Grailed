---
title: "Web Scraping Grailed"
output: html_document
date: '2023-07-05'
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

# Intro

This is a quick TLDR for how the code works.

## Libraries used
```{r library, warning = FALSE, message = FALSE, error = FALSE}
library(RSelenium)
library(tidyverse)
library(netstat)  
library(rvest)    
library(xml2)  
```

## Setting up Selenium

```{r sel, warning = FALSE, message = FALSE, error = FALSE, eval = FALSE}
drive_object2 <- rsDriver(
browser = "chrome",
chromever = "114.0.5735.90",
verbose = FALSE,
port = free_port())

rmDr2 <- drive_object2$client
rmDr2$maxWindowSize()
rmDr2$navigate("https://www.grailed.com/designers/reese-cooper")
```

---
  
![**Grailed Website:** Running the code should take you to the Grailed website](C:\Users\valen\OneDrive\Documents\grailed_project\Screenshot (144).png)

## Logging In  
In order to use Grailed, you must have an account. This process will automate logging in and selecting the sold filter.  

```{r login, eval = FALSE}
rmDr2$findElement(using = "id", "global-header-login-btn")$clickElement()
rmDr2$findElement(using = "xpath", "//button[@data-cy = 'login-with-email']")$clickElement()
email <- rmDr2$findElement(using = "id", "email")
email$sendKeysToElement(list('************@gmail.com'))
pass <- rmDr2$findElement(using =  "id", "password")
pass$sendKeysToElement(list('*********'))
rmDr2$findElement(using = "xpath", "//button[@data-cy = 'auth-login-submit']")$clickElement()

## Select Sold Items
rmDr2$findElement(using = "xpath", "(//div[starts-with(@class, 'instant-search-collapsible')])[9]")$clickElement()
checkbox <- rmDr2$findElement(using = "id", "sold-filter")
rmDr2$executeScript("arguments[0].click();", list(checkbox))

```

## Get Item Listings  
Each item shares the same class, "feed-item". To get every feed item, all the listings need to be viewed. This is done by scrolling through the entire page and selecting the correct xpath.  

![**Item listings**](C:\Users\valen\OneDrive\Documents\grailed_project\Screenshot (146).png)
---
```{r scroll, eval = FALSE}

# scroll to the bottom of the page
scroll_to_bottom <- function(driver) {
  driver$executeScript("window.scrollTo(0, document.body.scrollHeight);")
  Sys.sleep(5)  # Add a short delay to allow content to load
}

# Scroll until no more new content is loaded
previous_height <- -1  # Initialize the previous scroll height
current_height <- as.numeric(rmDr2$executeScript("return document.body.scrollHeight;"))

while (previous_height < current_height) {

  scroll_to_bottom(rmDr2)
  
  previous_height <- current_height
  current_height <- as.numeric(rmDr2$executeScript("return document.body.scrollHeight;"))
  
  Sys.sleep(5)
}

# Collect all feed items
links <- rmDr2$findElements(using = "xpath", "//div[@class = 'feed-item']")

# Verify listing number is correct
links %>% length()

# Collects the URL from each feed item, allowing us to scrape more data from the individual post.
web_link <- lapply(links, function(link) {
  childElements <- link$findChildElement(using = "xpath", ".//a")
  childElements$getElementAttribute("href")
})

# Unlist links
web_link <- web_link %>% unlist()
```

## Collecting data  

The scraping process is done through a for-loop, where "i" is for every item URL. The variables of interest are brand name, item name, number of likes, condition, color, size, price, image length, measurements, and description. Below is an example of what the code would look like for scraping sold outerwear items.

```{r scrape, eval = FALSE}
ow_df <- data.frame()

for (i in ow_link){
  
  rmDr2$navigate(i)
  
  Sys.sleep(3)
  
  brand_name <- rmDr2$findElement(using = "xpath", "//p[starts-with(@class, 'Headline_headline__')]")$getElementText() %>%
    unlist()
  
  item_name <- rmDr2$findElement(using = "xpath", "//h1[starts-with(@class, 'Body_body__')]")$getElementText() %>%
    unlist()
  
  number_likes <- tryCatch({
    likes <- rmDr2$findElement(using = "xpath", "//span[contains(@class, 'Likes_count__lMavB')]")$getElementText() %>%
      unlist()
    likes
  }, error = function(err) {
    return(0)
  })
  
  condition <- rmDr2$findElement(using = "xpath", "(//p[starts-with(@class, 'Body_body__dIg1V')])[4]")$getElementText() %>%
    unlist()
  
  color <- rmDr2$findElement(using = "xpath", "(//p[starts-with(@class, 'Body_body__dIg1V')])[3]")$getElementText() %>%
    unlist()
  
  size <- rmDr2$findElement(using = "xpath", "(//p[starts-with(@class, 'Body_body__dIg1V')])[1]")$getElementText() %>%
    unlist()
  
  price <- rmDr2$findElement(using = "xpath", "//*[starts-with(@class, 'Money_root__8lDCT')]")$getElementText() %>%
    unlist()
  
  img_length <- rmDr2$findElements(using = "xpath", "//img[starts-with(@class, 'Thumbnails_thumbnail__ixtme')]") %>% length()
  
  # Attempt to find the element
  measurements <- tryCatch({
    element <- rmDr2$findElement(using = "xpath", "//div[@class = 'Table_table__conFW']")
    TRUE  # Element found
  }, error = function(err) {
    FALSE  # Element not found
  })
  
  description_elements <- rmDr2$findElements(using = "xpath", "//p[contains(@class, 'ListingPage-Description-Body-Paragraph')]")
  
  description <- map(description_elements, ~ get_text(.)) %>% unlist() %>% paste(collapse = "\n")
  
  current_obs <- data.frame(brand_name,
                            item_name,
                            number_likes,
                            condition,
                            color,
                            size,
                            price,
                            img_length,
                            measurements,
                            description)
  
  ow_df <- rbind(ow_df, current_obs)
  
  Sys.sleep(2)
}
```






